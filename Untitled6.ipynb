{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6BxCWIoCbcVVB5M/U+tFh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irina-oss/NewRepo17/blob/master/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYD8LdjTNjBn",
        "outputId": "1097344c-de93-4160-9349-6757ac1c7fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install pypdf\n",
        "!pip install PyPDF2\n",
        "import os\n",
        "import urllib.request\n",
        "import textwrap\n",
        "from tqdm import tqdm\n",
        "from llama_cpp import Llama\n",
        "from PyPDF2 import PdfReader  # Импортируем класс PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Загрузка текста из PDF-файлов\n",
        "def load_pdf_text(lecture_file_paths):\n",
        "    \"\"\"Загружает текст из PDF-файла.\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(lecture_file_paths)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Ошибка: Файл не найден по пути {lecture_file_paths}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при чтении PDF-файла: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_all_lecture_texts(file_paths):\n",
        "    \"\"\"Загружает тексты из нескольких PDF-файлов.\"\"\"\n",
        "    all_texts = []\n",
        "    for lecture_file_paths in file_paths:\n",
        "        text = load_pdf_text(lecture_file_paths)\n",
        "        if text:\n",
        "            all_texts.append(text)\n",
        "    return all_texts\n",
        "\n",
        "\n",
        "# 2. Подготовка данных (разбивка на части)\n",
        "def prepare_data(text, chunk_size=512):\n",
        "    \"\"\"Разбивает текст лекции на небольшие фрагменты.\"\"\"\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "SkO5cJjI0Mhl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачиваем модель (если она еще не скачана)\n",
        "model_name = \"mistral-7b-openorca.gguf2.Q4_0.gguf\"\n",
        "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q4_0.gguf\" #Замените на работающий URL\n",
        "model_path = \"./\" + model_name\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Downloading {model_name} from {model_url}\")\n",
        "    try:\n",
        "        req = urllib.request.Request(model_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        response = urllib.request.urlopen(req)\n",
        "\n",
        "        total_size = int(response.headers.get('Content-Length', 0))\n",
        "        block_size = 8192\n",
        "\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=model_name) as t, open(model_path, 'wb') as outfile:\n",
        "            while True:\n",
        "                chunk = response.read(block_size)\n",
        "                if not chunk:\n",
        "                    break\n",
        "                outfile.write(chunk)\n",
        "                t.update(len(chunk))\n",
        "\n",
        "        print(f\"{model_name} downloaded to {model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "else:\n",
        "    print(f\"{model_name} already exists at {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oJR_9UwNkAg",
        "outputId": "9ff93d0e-6796-4810-8b19-9f31f467ab3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mistral-7b-openorca.gguf2.Q4_0.gguf already exists at ./mistral-7b-openorca.gguf2.Q4_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    llm = Llama(model_path=model_path, n_ctx=2048)  # Важно: Установите n_ctx (context window size)\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при инициализации Llama: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6rN9_zFNqec",
        "outputId": "895371d6-ac9f-4aa1-b938-c1ba5f8e189e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./mistral-7b-openorca.gguf2.Q4_0.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = open-orca_mistral-7b-openorca\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 3.83 GiB (4.54 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 0.1637 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.24 B\n",
            "print_info: general.name     = open-orca_mistral-7b-openorca\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32002\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<dummy32000>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<dummy32000>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q4_0) (and 66 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:  CPU_AARCH64 model buffer size =  3744.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  3917.88 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
            "....\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'open-orca_mistral-7b-openorca', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = \"\""
      ],
      "metadata": {
        "id": "wcml_xHlNw1q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. \"Обучение\" модели (на самом деле, добавление контекста)\n",
        "def add_lecture_context(lecture_text, llm, chunk_size=512):  # Передаём llm как аргумент\n",
        "    \"\"\"Добавляет текст лекции в контекст модели.\"\"\"\n",
        "    chunks = prepare_data(lecture_text, chunk_size)\n",
        "    for chunk in tqdm(chunks, desc=\"Adding lecture context\"):\n",
        "        #  Внимание! Важно:  Мы *не* обучаем модель в классическом понимании.\n",
        "        #  Мы просто добавляем текст лекции в *контекст* модели.\n",
        "        #  Это позволяет модели отвечать на вопросы, основываясь на этом тексте.\n",
        "\n",
        "        # Очистка контекста перед добавлением нового.  Если нужно накапливать знания, уберите эту строку.\n",
        "        #llm.reset()  #  Llama.cpp имеет метод reset()\n",
        "\n",
        "        #  Добавляем фрагмент текста в контекст. Используем простой промпт.\n",
        "        prompt = f\"Текст лекции: {chunk}\\nВопрос: Что это за текст?\\nОтвет:\"\n",
        "        try:\n",
        "            output = llm(prompt, max_tokens=50, stop=[\"Q:\", \"\\nUser:\", \"<|file_separator|>\"])\n",
        "            #  Мы можем игнорировать вывод, нас интересует, что текст добавлен в контекст.\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при добавлении контекста: {e}\")\n",
        "\n",
        "\n",
        "# 5. Замена `get_llama_response` для использования контекста лекции\n",
        "def get_llama_response(prompt, llm): # Передаём llm как аргумент\n",
        "    \"\"\"Получает ответ от модели с учетом контекста лекции.\"\"\"\n",
        "    try:\n",
        "        output = llm(prompt, max_tokens=200, stop=[\"Q:\", \"\\nUser:\", \"<|file_separator|>\"])  # Adjust parameters as needed\n",
        "        response_text = output['choices'][0]['text'].strip()\n",
        "        wrapped_text = textwrap.fill(response_text, width=100)\n",
        "        return wrapped_text\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при получении ответа от Llama: {e}\")\n",
        "        return \"Произошла ошибка при генерации ответа.\"\n",
        "\n",
        "# 6. Интеграция всего вместе и запуск\n",
        "if __name__ == \"__main__\":\n",
        "    lecture_file_paths = [\"/content/Лекция1.1.pdf\"]  # Замените на пути к вашим файлам лекций\n",
        "    all_lecture_texts = load_all_lecture_texts(lecture_file_paths)\n",
        "\n",
        "    if all_lecture_texts:\n",
        "        for lecture_text in all_lecture_texts:\n",
        "            add_lecture_context(lecture_text, llm) # Передаём llm\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"User: \")\n",
        "            if user_input.lower() == \"exit\":\n",
        "                break\n",
        "            prompt = f\"Q: {user_input}\\nAI: \"\n",
        "            response = get_llama_response(prompt, llm) # Передаём llm\n",
        "            print(\"AI:\", response)\n",
        "    else:\n",
        "        print(\"Не удалось загрузить текст лекции. Проверьте пути к файлам.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ULS1sl0ja6",
        "outputId": "639f06a5-a20c-46a9-db6c-17b5b9eed1b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAdding lecture context:   0%|          | 0/14 [00:00<?, ?it/s]Llama.generate: 8 prefix-match hit, remaining 233 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   63902.95 ms /   233 tokens (  274.26 ms per token,     3.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31411.15 ms /    49 runs   (  641.04 ms per token,     1.56 tokens per second)\n",
            "llama_perf_context_print:       total time =   95342.02 ms /   282 tokens\n",
            "Adding lecture context:   7%|▋         | 1/14 [01:35<20:39, 95.35s/it]Llama.generate: 8 prefix-match hit, remaining 244 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   65992.60 ms /   244 tokens (  270.46 ms per token,     3.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =   30914.73 ms /    49 runs   (  630.91 ms per token,     1.59 tokens per second)\n",
            "llama_perf_context_print:       total time =   96934.27 ms /   293 tokens\n",
            "Adding lecture context:  14%|█▍        | 2/14 [03:12<19:15, 96.28s/it]Llama.generate: 8 prefix-match hit, remaining 248 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   67656.41 ms /   248 tokens (  272.81 ms per token,     3.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12258.40 ms /    19 runs   (  645.18 ms per token,     1.55 tokens per second)\n",
            "llama_perf_context_print:       total time =   79925.47 ms /   267 tokens\n",
            "Adding lecture context:  21%|██▏       | 3/14 [04:32<16:17, 88.82s/it]Llama.generate: 8 prefix-match hit, remaining 240 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   65258.14 ms /   240 tokens (  271.91 ms per token,     3.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31627.09 ms /    49 runs   (  645.45 ms per token,     1.55 tokens per second)\n",
            "llama_perf_context_print:       total time =   96912.53 ms /   289 tokens\n",
            "Adding lecture context:  29%|██▊       | 4/14 [06:09<15:20, 92.02s/it]Llama.generate: 8 prefix-match hit, remaining 267 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   72088.84 ms /   267 tokens (  270.00 ms per token,     3.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31542.09 ms /    49 runs   (  643.72 ms per token,     1.55 tokens per second)\n",
            "llama_perf_context_print:       total time =  103659.50 ms /   316 tokens\n",
            "Adding lecture context:  36%|███▌      | 5/14 [07:52<14:25, 96.22s/it]Llama.generate: 8 prefix-match hit, remaining 220 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   58957.26 ms /   220 tokens (  267.99 ms per token,     3.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31356.25 ms /    49 runs   (  639.92 ms per token,     1.56 tokens per second)\n",
            "llama_perf_context_print:       total time =   90341.19 ms /   269 tokens\n",
            "Adding lecture context:  43%|████▎     | 6/14 [09:23<12:33, 94.22s/it]Llama.generate: 8 prefix-match hit, remaining 227 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   60694.34 ms /   227 tokens (  267.38 ms per token,     3.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31540.79 ms /    49 runs   (  643.69 ms per token,     1.55 tokens per second)\n",
            "llama_perf_context_print:       total time =   92265.28 ms /   276 tokens\n",
            "Adding lecture context:  50%|█████     | 7/14 [10:55<10:55, 93.58s/it]Llama.generate: 8 prefix-match hit, remaining 269 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   72384.17 ms /   269 tokens (  269.09 ms per token,     3.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31507.19 ms /    49 runs   (  643.00 ms per token,     1.56 tokens per second)\n",
            "llama_perf_context_print:       total time =  103918.83 ms /   318 tokens\n",
            "Adding lecture context:  57%|█████▋    | 8/14 [12:39<09:41, 96.88s/it]Llama.generate: 8 prefix-match hit, remaining 221 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   59462.72 ms /   221 tokens (  269.06 ms per token,     3.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31562.21 ms /    49 runs   (  644.13 ms per token,     1.55 tokens per second)\n",
            "llama_perf_context_print:       total time =   91052.95 ms /   270 tokens\n",
            "Adding lecture context:  64%|██████▍   | 9/14 [14:10<07:55, 95.06s/it]Llama.generate: 8 prefix-match hit, remaining 225 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   59811.82 ms /   225 tokens (  265.83 ms per token,     3.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31249.66 ms /    49 runs   (  637.75 ms per token,     1.57 tokens per second)\n",
            "llama_perf_context_print:       total time =   91088.67 ms /   274 tokens\n",
            "Adding lecture context:  71%|███████▏  | 10/14 [15:41<06:15, 93.83s/it]Llama.generate: 8 prefix-match hit, remaining 231 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   62887.96 ms /   231 tokens (  272.24 ms per token,     3.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =   30889.44 ms /    49 runs   (  630.40 ms per token,     1.59 tokens per second)\n",
            "llama_perf_context_print:       total time =   93803.56 ms /   280 tokens\n",
            "Adding lecture context:  79%|███████▊  | 11/14 [17:15<04:41, 93.83s/it]Llama.generate: 9 prefix-match hit, remaining 223 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   61460.62 ms /   223 tokens (  275.61 ms per token,     3.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =   30825.01 ms /    49 runs   (  629.08 ms per token,     1.59 tokens per second)\n",
            "llama_perf_context_print:       total time =   92312.48 ms /   272 tokens\n",
            "Adding lecture context:  86%|████████▌ | 12/14 [18:47<03:06, 93.37s/it]Llama.generate: 8 prefix-match hit, remaining 244 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   65896.88 ms /   244 tokens (  270.07 ms per token,     3.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =   31257.61 ms /    49 runs   (  637.91 ms per token,     1.57 tokens per second)\n",
            "llama_perf_context_print:       total time =   97181.60 ms /   293 tokens\n",
            "Adding lecture context:  93%|█████████▎| 13/14 [20:24<01:34, 94.53s/it]Llama.generate: 8 prefix-match hit, remaining 205 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   56268.08 ms /   205 tokens (  274.48 ms per token,     3.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =   30640.32 ms /    49 runs   (  625.31 ms per token,     1.60 tokens per second)\n",
            "llama_perf_context_print:       total time =   86937.56 ms /   254 tokens\n",
            "Adding lecture context: 100%|██████████| 14/14 [21:51<00:00, 93.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Что такое Информация?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =    5997.98 ms /    14 tokens (  428.43 ms per token,     2.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =   93256.83 ms /   148 runs   (  630.11 ms per token,     1.59 tokens per second)\n",
            "llama_perf_context_print:       total time =   99357.72 ms /   162 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: Информация является данными, фактами или знаниями, собираемыми, обрабатываемыми и предоставляемыми\n",
            "для использования. Информация может быть хранимой в виде текста, изображений, аудио или видео, или\n",
            "быть представлена в виде статистики, диаграмм или графиков. Ее основная цель - это предоставление\n",
            "информации, чтобы позволить пользователям принимать решения, расширять свои знания или выполнять\n",
            "другие полезные действия.\n",
            "User: что такое информационные технологии?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =    6098.63 ms /    15 tokens (  406.58 ms per token,     2.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =   82365.29 ms /   131 runs   (  628.74 ms per token,     1.59 tokens per second)\n",
            "llama_perf_context_print:       total time =   88552.53 ms /   146 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: Информационные технологии (ИТ) представляют собой обширную область, объединяющую различные аспекты\n",
            "использования компьютеров и других технологий для хранения, обработки, доступ к и обмен информацией.\n",
            "Это включает в себя программное обеспечение, аппаратное обеспечение, сетевые технологии, интернет,\n",
            "мобильные технологии, искусственный интеллект и многое другое.\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llama_response(prompt):\n",
        "    global conversation_history  # Объявляем, что используем глобальную переменную\n",
        "\n",
        "    # Добавляем историю разговора в запрос\n",
        "    full_prompt = conversation_history + prompt\n",
        "\n",
        "    try:\n",
        "        output = llm(full_prompt, max_tokens=200, stop=[\"Q:\", \"\\nUser:\", \"<|file_separator|>\"])  # Adjust parameters as needed\n",
        "        response_text = output['choices'][0]['text'].strip()\n",
        "\n",
        "        # Обновляем историю разговора\n",
        "        conversation_history += prompt + response_text + \"\\n\"\n",
        "\n",
        "        # Форматируем вывод с использованием textwrap\n",
        "        wrapped_text = textwrap.fill(response_text, width=100)\n",
        "        return wrapped_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при получении ответа от Llama: {e}\")\n",
        "        return \"Произошла ошибка при генерации ответа.\""
      ],
      "metadata": {
        "id": "nRXKyfLKNxgE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Формируем запрос\n",
        "    prompt = f\"Q: {user_input}\\nAI: \"\n",
        "\n",
        "    # Получаем ответ от Llama\n",
        "    response = get_llama_response(prompt)\n",
        "    print(\"AI:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxaab8_bN0sK",
        "outputId": "887d536a-fbb9-430b-c093-fea8cd3ba84f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Что такое информация?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =    4381.61 ms /    14 tokens (  312.97 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =   45214.11 ms /    73 runs   (  619.37 ms per token,     1.61 tokens per second)\n",
            "llama_perf_context_print:       total time =   49646.98 ms /    87 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: Информация - это данные, знания или факты, которые передаются от одной или нескольких людей или\n",
            "источников к другому человеку или к системе. Информация помогает людям принимать решения, учиться и\n",
            "общаться друг с другом.\n",
            "User: Информация - это\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 82 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   67073.88 ms\n",
            "llama_perf_context_print: prompt eval time =   22909.54 ms /    82 tokens (  279.38 ms per token,     3.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =   40835.04 ms /    65 runs   (  628.23 ms per token,     1.59 tokens per second)\n",
            "llama_perf_context_print:       total time =   63780.82 ms /   147 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: 1. Данные, знания или факты, которые передаются от одной или нескольких людей или источников к\n",
            "другому человеку или к системе. 2. Признание или утверждение о чем-то или кем-то.\n",
            "User: exit\n"
          ]
        }
      ]
    }
  ]
}